"""
scripts/fine_tune_lora.py
--------------------------------
Ce script effectue le fine-tuning d‚Äôun mod√®le de langage de base (Tiny GPT-2 ici pour test).
Dans un environnement restreint, il simule le processus complet :
 - Chargement du mod√®le et du tokenizer
 - Pr√©paration du dataset d'entra√Ænement
 - Simulation du fine-tuning (ou entra√Ænement r√©el si GPU)
 - Sauvegarde du mod√®le fine-tun√©

‚öôÔ∏è Ce script peut √™tre remplac√© par une version LoRA/QLoRA r√©elle si tu as acc√®s √† Hugging Face.
"""

import os
import json
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import Dataset

# =========================================
# üîß CONFIGURATION
# =========================================
MODEL_ID = "sshleifer/tiny-gpt2"  # Mod√®le l√©ger offline pour tests
DATA_PATH = "data/processed/train.jsonl"
OUTPUT_DIR = "outputs/models/fine_tuned_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =========================================
# üß† CHARGEMENT DU MOD√àLE ET TOKENIZER
# =========================================
print(f"[INFO] Chargement du mod√®le {MODEL_ID}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_ID)

# =========================================
# üìö CHARGEMENT DU DATASET
# =========================================
if not os.path.exists(DATA_PATH):
    print(f"[WARN] Dataset non trouv√© √† {DATA_PATH}. Cr√©ation d‚Äôun dataset simul√©...")

    dummy_data = [
        {"prompt": "Analyse le rapport du secteur nord.", "response": "Le secteur nord pr√©sente une activit√© mod√©r√©e."},
        {"prompt": "Quels sont les risques identifi√©s ?", "response": "Les risques incluent des tensions et des mouvements suspects."},
        {"prompt": "R√©sume la note de renseignement.", "response": "R√©sum√© : activit√© du groupe A sous surveillance."},
    ]
    os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)
    with open(DATA_PATH, "w", encoding="utf-8") as f:
        for item in dummy_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# =========================================
# ‚úÇÔ∏è PR√âPARATION DU DATASET
# =========================================
def tokenize_function(examples):
    inputs = [f"Question : {q['prompt']}\nR√©ponse :" for q in examples]
    targets = [q["response"] for q in examples]
    full_texts = [i + " " + t for i, t in zip(inputs, targets)]
    return tokenizer(full_texts, truncation=True, padding="max_length", max_length=256)

texts = [{"prompt": d["prompt"], "response": d["response"]} for d in data]
dataset = Dataset.from_list(texts)
tokenized_dataset = dataset.map(lambda x: tokenize_function([x])[0])

# =========================================
# üèãÔ∏è‚Äç‚ôÇÔ∏è CONFIGURATION D‚ÄôENTRA√éNEMENT (SIMUL√â)
# =========================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,  # simulation rapide
    per_device_train_batch_size=2,
    logging_dir="outputs/logs",
    logging_steps=5,
    save_steps=10,
    learning_rate=5e-5,
    save_total_limit=1,
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# =========================================
# üßÆ ENTRA√éNEMENT (ou simulation)
# =========================================
try:
    print("[INFO] D√©marrage de l‚Äôentra√Ænement (simul√© ou r√©el)...")
    trainer.train()
except Exception as e:
    print(f"[WARN] Entra√Ænement complet non disponible : {e}")
    print("[INFO] Simulation d‚Äôun mod√®le fine-tun√© termin√©e.")

# =========================================
# üíæ SAUVEGARDE DU MOD√àLE
# =========================================
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"[SUCCESS] Mod√®le fine-tun√© sauvegard√© dans {OUTPUT_DIR}")

# =========================================
# üîç TEST RAPIDE DU MOD√àLE
# =========================================
test_prompt = "R√©sume la note de renseignement sur le secteur nord."
inputs = tokenizer(test_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print("\n--- Exemple de g√©n√©ration ---")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
